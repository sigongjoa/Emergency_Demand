{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8770312",
   "metadata": {},
   "source": [
    "# 2023_05_22\n",
    "\n",
    "\n",
    "### data cleaninng\n",
    "\n",
    "traffic data의 city에는 미국의 모든 city가 들어 있어서  이름이 중복되는 경우가 존재함    \n",
    "(아래 map을 보면 4 구역의 Long Beach가 나옴)  \n",
    "\n",
    "우리가 원하는거는 Long Beach시/군 에 대한 데이터가 필요한데 traffic data에서는  Long Beach 동도 같이 city에 들어가 있는 느낌  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67583373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1200\"\n",
       "            height=\"800\"\n",
       "            src=\"../Meeting_Img/Long%20Beach_5_old.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fe3a01b6fd0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame('../Meeting_Img/Long%20Beach_5_old.html', width=1200, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f7391f",
   "metadata": {},
   "source": [
    "이를 해결하기 위해서 미국의 cences 데이터를 이용해서 우리가 원하는 Los Angles 밑에 있는 Long Beach만 선택해서 데이터 생성    \n",
    "cences의 geohash와 traffic data의 geohash가 일치하는 경우에만 사용  \n",
    "이를 모든 도시에 적용  \n",
    "\n",
    "> 다음의 파일을 수정 : http://202.31.200.194:8888/notebooks/NPLAB-NAS/Members/SEO/Emergency_Demand/Traffic_Accient/DataProcessing/GenerateData_Traffic.ipynb\n",
    "\n",
    "\n",
    "이후에 기존의 html 파일과 비교해서 정상적으로 데이터가 생성되었는지 확인  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67cd55aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1200\"\n",
       "            height=\"800\"\n",
       "            src=\"../Traffic_Accient/GeoHash_html/Long Beach_5.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fe3a01b68e0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame('../Traffic_Accient/GeoHash_html/Long Beach_5.html', width=1200, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3698c90b",
   "metadata": {},
   "source": [
    "### 데이터 생성 순서\n",
    "\n",
    "1. EventId별 사고 데이터 전처리 \n",
    "> http://202.31.200.194:8888/notebooks/NPLAB-NAS/Members/SEO/Emergency_Demand/Traffic_Accient/DataProcessing/GenerateData_Traffic.ipynb \n",
    "\n",
    "2. 날씨 데이터 전처리\n",
    "> http://202.31.200.194:8888/notebooks/NPLAB-NAS/Members/SEO/Emergency_Demand/Traffic_Accient/DataProcessing/GenerateData_Weather.ipynb\n",
    "\n",
    "3. 사고 데이터 및 날씨 데이터 병합 \n",
    "> http://202.31.200.194:8888/notebooks/NPLAB-NAS/Members/SEO/Emergency_Demand/Traffic_Accient/DataProcessing/GenerateData_Merge_TrafficWeather.ipynb\n",
    "\n",
    "4. 결측치 추가 \n",
    "> http://202.31.200.194:8888/notebooks/NPLAB-NAS/Members/SEO/Emergency_Demand/Traffic_Accient/DataProcessing/GenerateData_insertIndex.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20b51df",
   "metadata": {},
   "source": [
    "###  데이터 리뷰\n",
    "\n",
    "* target값에 대한 데이터 리뷰  \n",
    "발생빈도 , 기간별 통계치 및 geohash별 발생 건수 확인  \n",
    "\n",
    "\n",
    "> Long Beach 5 HTML : http://202.31.200.70:8888/tree/NPLAB-NAS/Members/SEO/Emergency_Demand/Traffic_Accient/DataProcessing/DataReview_HTML/Long%20Beach_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7f8de17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"500px\"\n",
       "            src=\"../Traffic_Accient/DataProcessing/DataReview_HTML/Long Beach_5/Accient_each_geohash.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7ff308441fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"500px\"\n",
       "            src=\"../Traffic_Accient/DataProcessing/DataReview_HTML/Long Beach_5/Accient by Day of the Week for each GeoHash.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7ff308441490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"500px\"\n",
       "            src=\"../Traffic_Accient/DataProcessing/DataReview_HTML/Long Beach_5/Acceint Sum Map.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7ff308441fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"500px\"\n",
       "            src=\"../Traffic_Accient/DataProcessing/DataReview_HTML/Long Beach_5/Accient.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7ff308441490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"500px\"\n",
       "            src=\"../Traffic_Accient/DataProcessing/DataReview_HTML/Long Beach_5/Accient by Hour of the Day for each GeoHash.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7ff308441fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"500px\"\n",
       "            src=\"../Traffic_Accient/DataProcessing/DataReview_HTML/Long Beach_5/Accient by Time for each GeoHash with Days.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7ff308441490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"500px\"\n",
       "            src=\"../Traffic_Accient/DataProcessing/DataReview_HTML/Long Beach_5/Time Series for batch index encoder cont.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7ff308441fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"500px\"\n",
       "            src=\"../Traffic_Accient/DataProcessing/DataReview_HTML/Long Beach_5/Accient by Time for each GeoHash with Weekly.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7ff308441490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"500px\"\n",
       "            src=\"../Traffic_Accient/DataProcessing/DataReview_HTML/Long Beach_5/Accient by Time for each GeoHash with Month.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7ff308441fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"500px\"\n",
       "            src=\"../Traffic_Accient/DataProcessing/DataReview_HTML/Long Beach_5/Time Series for encoder target.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7ff308441490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"500px\"\n",
       "            src=\"../Traffic_Accient/DataProcessing/DataReview_HTML/Long Beach_5/Time Series for encoder real.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7ff308441fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"500px\"\n",
       "            src=\"../Traffic_Accient/DataProcessing/DataReview_HTML/Long Beach_5/Accient by Time for each GeoHash.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7ff308441490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"500px\"\n",
       "            src=\"../Traffic_Accient/DataProcessing/DataReview_HTML/Long Beach_5/Accient by Day of the Month for each GeoHash.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7ff308441fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from IPython.display import IFrame\n",
    "\n",
    "folder_path = '../Traffic_Accient/DataProcessing/DataReview_HTML/Long Beach_5'\n",
    "file_list = os.listdir(folder_path)\n",
    "\n",
    "for file_name in file_list:\n",
    "    if file_name.endswith('.html'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        display(IFrame(file_path, width=\"100%\", height=\"500px\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ef6bde",
   "metadata": {},
   "source": [
    "* dataset review\n",
    "\n",
    "dataset에 생성한 train data가 어떻게 들어가는지 확인인 및 weather data의 interpoloation 확인 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338bad16",
   "metadata": {},
   "source": [
    "### params tuning\n",
    "\n",
    "Long Beach 5를 기준으로 학습 진행  \n",
    "성능이 잘 안나오는 관계로 hparmas tuning을 진행  \n",
    "\n",
    "lr을 바꿔도 성능은 그대로 예측을 하지 못하고 있음   \n",
    "\n",
    "> lr : http://202.31.200.194:8888/notebooks/NPLAB-NAS/Members/SEO/Emergency_Demand/Traffic_Accient/TFT/BaseModel_Long%20Beach_5_lr.ipynb\n",
    "\n",
    "\n",
    "batch_size가 너무 커서 학습을 못하고 있는것 같아서 batch_size를 조절    \n",
    "시간이 너무 오래 걸리는 관계로 backend 서버 이용  \n",
    "\n",
    "> batch_size: http://202.31.200.12:10382/notebooks/thkim/SEO/Emergency_Demand/BaseModel_Long%20Beach_5_batch_size.ipynb\n",
    "\n",
    "터미널을 이용해서 mulit gpu 사용코드   \n",
    "pytorch 버전은 1.0.0이 되면서 기존의 코드를 약간 수정    \n",
    "pytorch-lighting을 ligthing으로 수정  \n",
    "\n",
    "```python\n",
    "# save_as_script.py\n",
    "import argparse\n",
    "import copy\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import MAE, SMAPE, PoissonLoss, QuantileLoss\n",
    "\n",
    "lr = 0.005\n",
    "hidden = 64\n",
    "atten_head = 4\n",
    "dropout = 0.1\n",
    "\n",
    "def main(batch_size):\n",
    "    data = pd.read_csv('Long Beach_5.csv')\n",
    "\n",
    "    max_prediction_length = 24\n",
    "    max_encoder_length = 24*7\n",
    "    training_cutoff = data[\"time_idx\"].max() - max_prediction_length\n",
    "\n",
    "    training = TimeSeriesDataSet(\n",
    "        data[lambda x : x.time_idx <= training_cutoff],\n",
    "        time_idx = \"time_idx\",\n",
    "        target = \"Accient\",\n",
    "        group_ids = ['GeoHash'],\n",
    "        min_encoder_length=max_prediction_length//2,  # keep encoder length long (as it is in the validation set)\n",
    "        max_encoder_length=max_encoder_length,\n",
    "        min_prediction_length=1,\n",
    "        max_prediction_length=max_prediction_length,\n",
    "        static_categoricals=[\"GeoHash\"],\n",
    "        time_varying_known_categoricals=[],\n",
    "        time_varying_known_reals=[],\n",
    "        #time_varying_unknown_categoricals=,\n",
    "        time_varying_unknown_reals=data.columns[4: -2].tolist(), \n",
    "        target_normalizer=GroupNormalizer(\n",
    "            groups=[\"GeoHash\"], \n",
    "            transformation=\"relu\",\n",
    "            center = False\n",
    "        ),\n",
    "        add_relative_time_idx=True,\n",
    "        add_target_scales=False,\n",
    "        add_encoder_length=True,\n",
    "        #allow_missing=True,\n",
    "        allow_missing_timesteps = True,\n",
    "        #predict_mode = False\n",
    "    )\n",
    "\n",
    "    validation = TimeSeriesDataSet.from_dataset(training, data, predict=True, stop_randomization=True)\n",
    "\n",
    "    train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=3)\n",
    "    val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=3)\n",
    "\n",
    "    logger = TensorBoardLogger(\"Long Baech_logs\")\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=50,\n",
    "        strategy=\"ddp\",\n",
    "        #accelerator='gpu', \n",
    "        devices=[0,1,2],\n",
    "        gradient_clip_val=0.1,\n",
    "        #detect_anomaly=True,\n",
    "    )\n",
    "\n",
    "\n",
    "    tft = TemporalFusionTransformer.from_dataset(\n",
    "        training,\n",
    "        learning_rate=lr,\n",
    "        hidden_size=hidden ,\n",
    "        attention_head_size=atten_head,\n",
    "        dropout=dropout,  \n",
    "        hidden_continuous_size=hidden,\n",
    "        output_size=7,  \n",
    "        loss=QuantileLoss(),\n",
    "        reduce_on_plateau_patience=4,\n",
    "    )\n",
    "\n",
    "    trainer.fit(\n",
    "        tft,\n",
    "        train_dataloaders=train_dataloader,\n",
    "        val_dataloaders=val_dataloader\n",
    "    )\n",
    "\n",
    "    print(trainer.checkpoint_callback.best_model_path)\n",
    "\n",
    "    raw_predictions, x , idx_df = tft.predict(val_dataloader, mode=\"raw\", return_x=True , return_index=True)\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=raw_predictions['prediction'].shape[0], figsize=(10, raw_predictions['prediction'].shape[0]*5))\n",
    "\n",
    "    for idx in range(raw_predictions['prediction'].shape[0]):  # plot examples\n",
    "        tft.plot_prediction(x, raw_predictions, idx=idx, add_loss_to_title=True, ax=axs[idx])\n",
    "\n",
    "    plt.savefig(f\"batch_size_{batch_size}figure.png\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='PyTorch Forecasting Script')\n",
    "    parser.add_argument('--batch_size', type=int, default=8, help='Batch size for training (default: 8)')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    main(args.batch_size)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbf5a2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
